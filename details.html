<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Details | GENEA Leaderboard </title> <meta name="author" content="GENEA Leaderboard "> <meta name="description" content="An overview of the leaderboard initiative"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/leaderboard/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/leaderboard/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/leaderboard/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/leaderboard/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/leaderboard/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://genea-workshop.github.io/leaderboard/details"> <script src="/leaderboard/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/leaderboard/"> <span class="font-weight-bold">GENEA Leaderboard</span> </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/leaderboard/">Home </a> </li> <li class="nav-item active"> <a class="nav-link" href="/leaderboard/details">Details <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/leaderboard/organisers/">Organising team </a> </li> <li class="nav-item "> <a class="nav-link" href="/leaderboard/position_paper">Position paper </a> </li> <li class="nav-item "> <a class="nav-link" href="/leaderboard/ranking">Leaderboard ranking </a> </li> <li class="nav-item "> <a class="nav-link" href="/leaderboard/submission">Submission </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <h1 style="color: #3979c0"> About the Leaderboard</h1> <p>The GENEA Gesture Generation Leaderboard is an upcoming living benchmark for gesture generation models. It allows researchers to test and showcase their models’ performance on a standardised dataset and evaluation methodology.</p> <p>Initially, the leaderboard will feature results from existing models that have been adapted to the BEAT-2 dataset. Following this initial phase, we’ll open the leaderboard to all researchers interested in submitting and evaluating new models.</p> <h2>Our goals</h2> <ul> <li>Establish a continuously updated <b>definitive ranking</b> of state-of-the-art models <b>on the most common</b> speech-gesture <b>motion capture datasets</b>, based on human evaluation.</li> <li> Raise the standards for <b>reproducibility</b> by releasing all collected model outputs, human ratings, and scripts for motion visualisation, conducting user-studies, and more. </li> <li> Use the collected human ratings to develop <b>better objective metrics</b> that are aligned with human perception of motion; </li> <li> <b>Unify gesture-generation researchers</b> from computer vision, computer graphics, machine learning, NLP, HCI, and robotics. </li> <li> <b>Evolve with the community</b> as new datasets, evaluations, and methodologies become available. </li> </ul> <h2>Outcomes</h2> <p>Once the Leaderboard is operational, you will be able to:</p> <ul> <li> <b>Submit</b> your gesture-generation model's outputs, and <b>receive human evaluation results in 2-4 weeks for free</b>, managed by our expert team; </li> <li> <b>Compare</b> to any state-of-the-art method on the Leaderboard using our comprehensive collection of rendered video outputs, <b>without having to reproduce baselines</b>; </li> <li> <b>Visualise</b> your generated motion and <b>conduct our user studies</b> on your own <b>using our easy-to-use open-source tools</b>; </li> <li> ...and much more! </li> </ul> <h1 style="color: #3979c0">Setup and development timeline</h1> <p>To construct the leaderboard, we are inviting authors of gesture-generation models published in recent years to participate in a large-scale evaluation. We will conduct comprehensive evaluation of the submitted systems, primarily based on human evaluation, which will then be published on our website, alongside all collected outputs, ratings, and scripts necessary for reproducing the evaluation.</p> <p>Afterwards, the leaderboard will become open to the public in <strong>June 2025</strong>, and we will be updating it continuously as we receive new model submissions.</p> <h1 style="color: #3979c0">Dataset</h1> <h3>BEAT-2 in the SMPL-X Format</h3> <p>The leaderboard will initially evaluate models using the English recordings in the test split of the <a href="https://pantomatrix.github.io/EMAGE/" rel="external nofollow noopener" target="_blank">BEAT-2 dataset</a>. Submissions will be required to be in the same SMPL-X format as the dataset, but we will <b>hide facial expressions</b> in order to focus on the hand- and body movements.</p> <figure style="text-align: center; margin-bottom: 2em;"> <video width="80%" controls=""> <source src="./assets/video/smplx_render_example.mp4" type="video/mp4"></source> ERROR: Your browser could not load the example video. </video> <figcaption><i>An example video clip rendered from the BEAT-2 dataset. The avatar is a textured <a href="https://smpl-x.is.tue.mpg.de/" rel="external nofollow noopener" target="_blank">SMPL-X mesh</a>.</i></figcaption> </figure> <p>We think this data is the best candidate for an initial benchmark dataset for several reasons:</p> <ol> <li>It’s the largest public mocap dataset of gesturing (with 60 hours of data in total).</li> <li>BEAT, its predecessor, is one of the most commonly used gesture-generation datasets in recent years.</li> <li>It has a high variety of speakers and emotions, and it includes semantic gesture annotations.</li> <li>The SMPL-X format is compatible with many other mocap datasets, the majority of pose estimation models, and includes potential extensions (e.g., facial expressions for future iterations).</li> </ol> <p>Being a living leaderboard, the dataset used for benchmarking is expected to change in the future as better mocap datasets become available.</p> <h1 style="color: #3979c0">Submission process</h1> <h2>Rules for participation</h2> <p>The main goal of the GENEA Leaderboard is to advance the scientific field of automated gesture generation. To best achieve this, we have a few requirements for participation:</p> <ol style="display: grid; gap: 10px"> <li>We are primarily looking to evaluate established gesture generation systems (e.g., already published models). In practice, this means that we maintain the right to filter out submissions that do not have prior evaluation results, or are clearly far below the state-of-the-art.</li> <li>By participating in the leaderboard, you allow us to share your submitted motion clips with the scientific community, alongside with the videos we render from them and the human preference data collected during the evaluation.</li> <li>You may train your model on any dataset that is publicly available for research purposes, except for the test set of BEAT-2, or the corresponding files in the original BEAT dataset. Training on any data not publicly available for research purposes is strictly prohibited.</li> <li>In order to be included in the evaluation, you will have to provide a technical report describing the details of your system and how you trained it. Note that if your submission is based on an already published system, you will only have to write about how you adapted it for the leaderboard.</li> </ol> <h2>How to participate</h2> <ol style="display: grid; gap: 10px"> <li> <b>Pre-screening</b>: Send an e-mail to our <a href="mailto:%20genea-leaderboard@googlegroups.com">contact address</a>, including the following information: <ul> <li>which model you intend to submit to the evaluation,</li> <li>a link to a document describing said model (e.g., a paper),</li> <li>a rough outline of planned changes to the original model (if any),</li> <li>a list of team members contributing to your adaptation of the model, and</li> <li>the name of a main responsible for the team.</li> </ul> We will get back to you within a few days, confirming whether we can commit to evaluating your future submission.</li> <li> <b>Prepare your model</b>: Train your model on the official training split of the BEAT-2 dataset and/or any other publicly available mocap data, except for the BEAT-2 test set. Given an arbitrarily long speech recording, your model must be able to generate an equally long motion sequence. Speaker ID, emotion labels, and the SMPL-X body shape vector will always be available as inputs, but you are not required to use them.</li> <li> <b>Generate your synthetic motion</b>: For each speech recording in the BEAT-2 English test set, generate corresponding synthetic movements in the SMPL-X format. If your model is probabilistic (i.e., nondeterministic), please generate 5 samples for each input file, so that we can measure diversity.</li> <li> <b>Submit your motion data and write a technical report:</b> Send your motion data to us, either through e-mail or on the submission page we will prepare for you. In order for your submission to be included on the leaderboard, you have to commit to writing a technical report about your submission, including but not limited to details about data, model architecture, training methods and generation process.</li> </ol> <h2>What happens after your submission</h2> <ol style="display: grid; gap: 10px"> <li> <b>Submission screening</b>: Our team will inspect your submitted motion in order to validate whether your results are suitable for the leaderboard. We will get back to you within a week, and we will only reject submission in exceptional cases (e.g., if the movements are extremely jerky or still). </li> <li> <b>Clip segmentation</b>: We will split your submitted motion sequences into short evaluation clips (roughly 10-15 seconds each). We will take care that the evaluation clips are aligned with full speaking turns. The timestamps of the evaluation clips will be kept private in order to prevent cherry-picking.</li> <li> <b>Video rendering</b>: Our research engineers will create high-quality close-up video renders of the evaluation clips, using a standardised 3D scene and a textured SMPL-X character model. </li> <li> <b>Crowd-sourced evaluations</b>: Once we have received enough submissions, we will conduct rigorous large-scale user studies, as detailed below on this page, and perform statistical analysis on the results. </li> <li> <b>Release of data and evaluation results</b>: We will update the leaderboard based on the statistical results, and publish all of your rendered videos alongside your technical report. </li> <li> <b>State-of-the-art report</b>: Periodically, we will invite participating teams to co-author a detailed state-of-the-art report, based on a snapshot of the leaderboard, in the style of the GENEA Challenge papers.</li> </ol> <h1 style="color: #3979c0">Evaluation methodology</h1> <p>We will recruit large numbers of evaluators to conduct best-practices human evaluation. Our perceptual user studies will be designed to carefully disentangle key aspects of gesture evaluation, following what we learned from organising the <a href="https://linktr.ee/genea_workshop" rel="external nofollow noopener" target="_blank">2020–2023 GENEA challenges</a>.</p> <h2 style="color: #3979c0">Evaluation tasks</h2> <h3>Motion quality</h3> <p>The first evaluation task will measure motion quality, in other words, to what degree do the evaluators perceive the overall movements to be natural-looking gesturing, without considering the speech. For this evaluation, the stimuli will be <b>silent videos</b>, and we will perform pairwise comparisons of motions from different sources (e.g., gesture-generation systems, baselines, or mocap data).</p> <p>The statistical analysis will use an Elo-style ranking system, in particular the Bradley-Terry model, similar to the methodology of <a href="https://lmarena.ai/" rel="external nofollow noopener" target="_blank">Chatbot Arena</a>. You can read more about Elo scores and the Bradley-Terry model in <a href="https://lmsys.org/blog/2023-12-07-leaderboard/#transition-from-online-elo-rating-system-to-bradley-terry-model" rel="external nofollow noopener" target="_blank">this blog post</a>; the key point is that 1) Elo-like systems naturally work well in a leaderboard setting where scores are continuously updated and the comparisons are not necessarily exhaustive; 2) the difference between the Elo scores of two systems directly expresses the probability that users prefer the output of one system over the output of the other.</p> <p>We believe that this approach will prove to be a highly scalable and efficient method, with interpretable results, that allows us to conduct sustainable recurring evaluations for each future submission separately.</p> <figure style="text-align: center; margin-bottom: 2em;"> <img src="./assets/img/hemvip_gui.png" style="border: 1px solid black;" width="100%"> <figcaption><i>A preview of the evaluation interface used in our studies.</i></figcaption> </figure> <h3>Motion specificity to speech</h3> <p>The second evaluation task will measure whether the outputs of the gesture-generation system are <i>somehow related</i> to the speech input. As discussed in the <a href="https://dl.acm.org/doi/10.1145/3656374" rel="external nofollow noopener" target="_blank">GENEA challenge 2022 paper</a>, a naive evaluation of this question – e.g., directly asking evaluators to choose which of two systems generated movements that are more appropriate for the speech – has significant risk of confounding with other factors such as motion quality.</p> <p>Therefore, we will use a mismatching procedure based on the <a href="https://arxiv.org/abs/2308.12646" rel="external nofollow noopener" target="_blank">GENEA Challenges</a>. In a nutshell, our approach will be to show two clips from the same system, one with correctly paired speech and motion, and the other with independent, intentionally misaligned motion and speech signals. Evaluators then will be tasked with indicating which of the two videos has better connection between speech and motion.</p> <p>This is also a pairwise comparison, but unlike the motion quality assessment, it can be performed for each system independently, therefore it avoids the confounding factor of motion quality.</p> <h3>Future evaluations</h3> <p>After the leaderboard becomes established, we will include new evaluation tasks based on what datasets become available, and what challenges become more important in the field. Some possibilities, already compatible with the BEAT2 dataset, are to evaluate facial expressions or emotion expressivity. For future datasets, it might become possible to test motion specificity to the meaning of the speech, and other types of grounding information. (See our <a href="/leaderboard/position_paper">position paper for more ideas.)</a></p> <h1 style="color: #3979c0">Tooling</h1> <h2>Standardised visualisation</h2> <p>Visualisation is one of the most important design choices for perceptual user studies that evaluate motion synthesis. Currently, almost every gesture-generation paper uses a different character model and 3D scene configuration due to difficulties of using animation software, as well as the lack of shared 3D assets. Because character appearence and other environmental factors can have a subtle but important effect on the evaluation, this means that human evaluation results are largely incomparable to each other.</p> <p>We will use a standardised visualisation setup, containining a textured SMPl-X mesh as a human character model, and a minimal 3D scene with lighting. There will be an option to hide the face in the videos, since our first evaluations will only be based on hand- and body motion. <strong>The videos shown above on this page are previews of our visualisation setup.</strong></p> <p>We are currently working on an open-source, automated pipeline for rendering videos for our user studies, based on our previous <a href="https://github.com/TeoNikolov/genea_visualizer/" rel="external nofollow noopener" target="_blank">GENEA Blender visualiser</a>. The updated pipeline will be shared with the community after we release the leaderboard.</p> <h2>User-study automation</h2> <p>To standardise the human evaluation process, we are rewriting the <a href="https://github.com/genea-workshop/hemvip" rel="external nofollow noopener" target="_blank">HEMVIP codebase</a>, which was originally developed for the GENEA challenges, with an emphasis on stability and ease of use. This software will also be open-sourced – our vision is to enable independent replication of our evaluations, and to lower the barriers for crowd-sourced evaluations.</p> <h2>Objective evaluation</h2> <p>The leaderboard will also feature many commonly used objective metrics (e.g., FGD and beat consistency), and we are planning to develop new automated evaluation methods based on the collected human preference data. Each of these will be open-sourced with the release of the leaderboard.</p> <h1 style="color: #3979c0">Frequently Asked Questions</h1> <details> <summary>Why do we need a leaderboard?</summary> <ul> <li>Gesture generation research is currently fragmented across different datasets and evaluation protocols.</li> <li>Objective metrics are inconsistently applied, and their validity is not sufficiently established in the literature.</li> <li>At the same time, subjective evaluation methods often have low reproducibility, and their results are impossible to directly compare to each other.</li> <li>This leads to a situation where it is impossible to know what is the state of the art, or to know which method works better for which purpose when comparing two publications.</li> <li>The leaderboard is designed to directly counter these issues.</li> </ul> </details> <details> <summary>How are the evaluations funded?</summary> <p>We currently have academic funding for running the leaderboard for a period of time. Having your system evaluated by the leaderboard will be free of charge. However, if there are a lot of systems submitted, we might not be able to evaluate all of them.</p> </details> <p><br></p> <h1 align="left" style="color: #3979c0">Contact address</h1> <p style=" font-size: 2em; text-align: center"><a style="color: #000000" href="mailto:%20genea-leaderboard@googlegroups.com">genea-leaderboard@googlegroups.com</a></p> </div> </div> </div> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/leaderboard/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/leaderboard/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/leaderboard/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/leaderboard/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/leaderboard/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/leaderboard/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/leaderboard/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/leaderboard/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>