import"../chunks/DsnmJJEf.js";import"../chunks/BRC1R0o8.js";import{p as v,f as w,t as y,a as k,b as x,d as e,s as h,n as a,r as t}from"../chunks/BM_dKhXI.js";import{r as d,s as u}from"../chunks/XfYtYKOx.js";import{i as E}from"../chunks/BZ7KvBYk.js";var T=w(`<main class="subcontainer"><div class="home-split-ver"><div class="card"><h1 class="h1" style="color: #3979c0">About the Leaderboard</h1> <p>The GENEA Gesture Generation Leaderboard is an upcoming living benchmark for gesture generation models. It allows researchers to test and showcase their models’ performance on a standardised dataset and evaluation methodology.</p> <p>Initially, the leaderboard will feature results from existing models that have been adapted to the BEAT-2 dataset. Following this initial phase, we’ll open the leaderboard to all researchers interested in submitting and evaluating new models.</p> <h2 class="h2">Our goals</h2> <ul><li>Establish a continuously updated <b>definitive ranking</b> of state-of-the-art models <b>on the most common</b> speech-gesture <b>motion capture datasets</b>, based on human evaluation.</li> <li>Raise the standards for <b>reproducibility</b> by releasing all collected model outputs, human ratings, and scripts for motion visualisation, conducting user-studies, and more.</li> <li>Use the collected human ratings to develop <b>better objective metrics</b> that are aligned with human perception of motion;</li> <li><b>Unify gesture-generation researchers</b> from computer vision, computer graphics, machine learning, NLP, HCI, and robotics.</li> <li><b>Evolve with the community</b> as new datasets, evaluations, and methodologies become available.</li></ul> <h2 class="h2">Outcomes</h2> <p>Once the Leaderboard is operational, you will be able to:</p> <ul><li><b>Submit</b> your gesture-generation model's outputs, and <b>receive human evaluation results in 2-4 weeks for free</b>, managed by our expert team;</li> <li><b>Compare</b> to any state-of-the-art method on the Leaderboard using our comprehensive collection of rendered video outputs, <b>without having to reproduce baselines</b>;</li> <li><b>Visualise</b> your generated motion and <b>conduct our user studies</b> on your own <b>using our easy-to-use open-source tools</b>;</li> <li>...and much more!</li></ul> <h1 class="h1" style="color: #3979c0">Setup and development timeline</h1> <p>To construct the leaderboard, we are inviting authors of gesture-generation models published in recent years to participate in a large-scale evaluation. We will conduct comprehensive evaluation of the submitted systems, primarily based on human evaluation, which will then be published on our website, alongside all collected outputs, ratings, and scripts necessary for reproducing the evaluation.</p> <p>Afterwards, the leaderboard will become open to the public in <strong>June 2025</strong>, and we will be updating it continuously as we receive new model submissions.</p> <h1 class="h1" style="color: #3979c0">Dataset</h1> <h3 class="h3">BEAT-2 in the SMPL-X Format</h3> <p>The leaderboard will initially evaluate models using the English recordings in the test split of the <a href="https://pantomatrix.github.io/EMAGE/" rel="external nofollow noopener" target="_blank">BEAT-2 dataset</a>. Submissions will be required to be in the same SMPL-X format as the dataset, but we will <b>hide facial expressions</b> in order to focus on the hand- and body movements.</p> <figure style="text-align: center; margin-bottom: 2em;"><video width="80%" controls><source type="video/mp4"/> ERROR: Your browser could not load the example video.</video> <figcaption><i>An example video clip rendered from the BEAT-2 dataset. The avatar is a textured <a href="https://smpl-x.is.tue.mpg.de/" rel="external nofollow noopener" target="_blank">SMPL-X mesh</a>.</i></figcaption></figure> <p>We think this data is the best candidate for an initial benchmark dataset for several reasons:</p> <ol><li>It’s the largest public mocap dataset of gesturing (with 60 hours of data in total).</li> <li>BEAT, its predecessor, is one of the most commonly used gesture-generation datasets in recent years.</li> <li>It has a high variety of speakers and emotions, and it includes semantic gesture annotations.</li> <li>The SMPL-X format is compatible with many other mocap datasets, the majority of pose estimation models, and includes potential extensions (e.g., facial expressions for future iterations).</li></ol> <p>Being a living leaderboard, the dataset used for benchmarking is expected to change in the future as better mocap datasets become available.</p> <h1 class="h1" style="color: #3979c0">Evaluation methodology</h1> <p>We will recruit large numbers of evaluators to conduct best-practices human evaluation. Our perceptual user studies will be designed to carefully disentangle key aspects of gesture evaluation, following what we learned from organising the <a href="https://linktr.ee/genea_workshop" rel="external nofollow noopener" target="_blank">2020–2023 GENEA challenges</a>.</p> <h2 class="h2" style="color: #3979c0">Evaluation tasks</h2> <h3 class="h3">Motion quality</h3> <p>The first evaluation task will measure motion quality, in other words, to what degree do the evaluators perceive the overall movements to be natural-looking gesturing, without considering the speech. For this evaluation, the stimuli will be <b>silent videos</b>, and we will perform pairwise comparisons of motions from different sources (e.g., gesture-generation systems, baselines, or mocap data).</p> <p>The statistical analysis will use an Elo-style ranking system, in particular the Bradley-Terry model, similar to the methodology of <a href="https://lmarena.ai/" rel="external nofollow noopener" target="_blank">Chatbot Arena</a>. You can read more about Elo scores and the Bradley-Terry model in <a href="https://lmsys.org/blog/2023-12-07-leaderboard/#transition-from-online-elo-rating-system-to-bradley-terry-model" rel="external nofollow noopener" target="_blank">this blog post</a>; the key point is that 1) Elo-like systems naturally work well in a leaderboard setting where scores are continuously updated and the comparisons are not necessarily exhaustive; 2) the difference between the Elo scores of two systems directly expresses the probability that users prefer the output of one system over the output of the other.</p> <p>We believe that this approach will prove to be a highly scalable and efficient method, with interpretable results, that allows us to conduct sustainable recurring evaluations for each future submission separately.</p> <figure style="text-align: center; margin-bottom: 2em;"><img style="border: 1px solid black;" width="100%"/> <figcaption><i>A preview of the evaluation interface used in our studies.</i></figcaption></figure> <h3 class="h3">Motion specificity to speech</h3> <p>The second evaluation task will measure whether the outputs of the gesture-generation system are <i>somehow related</i> to the speech input. As discussed in the <a href="https://dl.acm.org/doi/10.1145/3656374" rel="external nofollow noopener" target="_blank">GENEA challenge 2022 paper</a>, a naive evaluation of this question – e.g., directly asking evaluators to choose which of two systems generated movements that are more appropriate for the speech – has significant risk of confounding with other factors such as motion quality.</p> <p>Therefore, we will use a mismatching procedure based on the <a href="https://arxiv.org/abs/2308.12646" rel="external nofollow noopener" target="_blank">GENEA Challenges</a>. In a nutshell, our approach will be to show two clips from the same system, one with correctly paired speech and motion, and the other with independent, intentionally misaligned motion and speech signals. Evaluators then will be tasked with indicating which of the two videos has better connection between speech and motion.</p> <p>This is also a pairwise comparison, but unlike the motion quality assessment, it can be performed for each system independently, therefore it avoids the confounding factor of motion quality.</p> <h3 class="h3">Future evaluations</h3> <p>After the leaderboard becomes established, we will include new evaluation tasks based on what datasets become available, and what challenges become more important in the field. Some possibilities, already compatible with the BEAT2 dataset, are to evaluate facial expressions or emotion expressivity. For future datasets, it might become possible to test motion specificity to the meaning of the speech, and other types of grounding information. (See our <a href="https://arxiv.org/abs/2410.06327">position paper for more ideas.)</a></p> <h1 class="h1" style="color: #3979c0">Tooling</h1> <h2 class="h2">Standardised visualisation</h2> <p>Visualisation is one of the most important design choices for perceptual user studies that evaluate motion synthesis. Currently, almost every gesture-generation paper uses a different character model and 3D scene configuration due to difficulties of using animation software, as well as the lack of shared 3D assets. Because character appearence and other environmental factors can have a subtle but important effect on the evaluation, this means that human evaluation results are largely incomparable to each other.</p> <p>We will use a standardised visualisation setup, containining a textured SMPl-X mesh as a human character model, and a minimal 3D scene with lighting. There will be an option to hide the face in the videos, since our first evaluations will only be based on hand- and body motion. <strong>The videos shown above on this page are previews of our visualisation setup.</strong></p> <p>We are currently working on an open-source, automated pipeline for rendering videos for our user studies, based on our previous <a href="https://github.com/TeoNikolov/genea_visualizer/" rel="external nofollow noopener" target="_blank">GENEA Blender visualiser</a>. The updated pipeline will be shared with the community after we release the leaderboard.</p> <h2 class="h2">User-study automation</h2> <p>To standardise the human evaluation process, we are rewriting the <a href="https://github.com/genea-workshop/hemvip" rel="external nofollow noopener" target="_blank">HEMVIP codebase</a>, which was originally developed for the GENEA challenges, with an emphasis on stability and ease of use. This software will also be open-sourced – our vision is to enable independent replication of our evaluations, and to lower the barriers for crowd-sourced evaluations.</p> <h2 class="h2">Objective evaluation</h2> <p>The leaderboard will also feature many commonly used objective metrics (e.g., FGD and beat consistency), and we are planning to develop new automated evaluation methods based on the collected human preference data. Each of these will be open-sourced with the release of the leaderboard.</p> <h1 class="h1" style="color: #3979c0">Frequently Asked Questions</h1> <details><summary>Why do we need a leaderboard?</summary> <ul><li>Gesture generation research is currently fragmented across different datasets and evaluation protocols.</li> <li>Objective metrics are inconsistently applied, and their validity is not sufficiently established in the literature.</li> <li>At the same time, subjective evaluation methods often have low reproducibility, and their results are impossible to directly compare to each other.</li> <li>This leads to a situation where it is impossible to know what is the state of the art, or to know which method works better for which purpose when comparing two publications.</li> <li>The leaderboard is designed to directly counter these issues.</li></ul></details> <details><summary>How are the evaluations funded?</summary> <p>We currently have academic funding for running the leaderboard for a period of time. Having your system evaluated by the leaderboard will be free of charge. However, if there are a lot of systems submitted, we might not be able to evaluate all of them.</p></details> <p><br/></p> <h1 class="h1" style="color: #3979c0; align: left">Contact address</h1> <p style=" font-size: 2em; text-align: center"><a href="mailto:%20genea-leaderboard@googlegroups.com">genea-leaderboard@googlegroups.com</a></p></div></div></main>`,2);function L(c,p){v(p,!1),E();var o=T(),s=e(o),r=e(s),i=h(e(r),28),n=e(i),m=e(n);a(),t(n),a(2),t(i);var l=h(i,22),b=e(l);a(2),t(l),a(42),t(r),t(s),t(o),y((f,g)=>{u(m,"src",f),u(b,"src",g)},[()=>d("/smplx_render_example.mp4"),()=>d("/hemvip_gui.png")]),k(c,o),x()}export{L as component};
